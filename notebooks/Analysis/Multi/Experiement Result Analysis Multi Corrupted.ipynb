{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessory-identity",
   "metadata": {},
   "source": [
    "# Visualize Results: Downstream Performance - Multiclass Classification Corrupted Experiments -> Training and Test identically imputed\n",
    "\n",
    "[Set Average Best Imputation Method Manually](#Set-Average-Best-Imputation-Method-Manually)\n",
    "\n",
    "Notebook wurde angepasst -> für Tests nutzen!\n",
    "\n",
    "This notebook should answer the questions: *Does imputation lead to better downstream performances?*\n",
    "\n",
    "Data needs to be preprocessed with other notebook, her we only import two csv files with raw data regarding the results of the experiment and information about the used datasets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broad-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c5d0f-9168-4d56-85cf-0617e99670be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-province",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ecf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context('paper', font_scale=1.5)\n",
    "mpl.rcParams['lines.linewidth'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unlimited-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF_METRIC = \"F1_macro\"\n",
    "REG_METRIC = \"RMSE\"\n",
    "\n",
    "DOWNSTREAM_RESULT_TYPE = \"downstream_performance_mean\"\n",
    "IMPUTE_RESULT_TYPE = \"impute_performance_mean\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-introduction",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012aca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocessed data from experiments\n",
    "results = pd.read_csv('../corrupted_imputer_test_multi_autogluon.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61566728-578a-4aeb-982c-1d3d4e39800a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "severe-retreat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Petro\\AppData\\Local\\Temp\\ipykernel_2464\\3401284833.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  na_impute_results.drop([\"baseline\", \"corrupted\", \"imputed\"], axis=1, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the relevant data for downstream analysis\n",
    "\n",
    "na_impute_results = results[\n",
    "    (results[\"result_type\"] == IMPUTE_RESULT_TYPE) & \n",
    "    (results[\"metric\"].isin([\"F1_macro\", \"RMSE\"]))\n",
    "]\n",
    "na_impute_results.drop([\"baseline\", \"corrupted\", \"imputed\"], axis=1, inplace=True)\n",
    "na_impute_results = na_impute_results[na_impute_results.isna().any(axis=1)]\n",
    "na_impute_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "static-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if strategy type is correct!\n",
    "STRATEGY_TYPE = \"single_single\"\n",
    "\n",
    "downstream_results = results[\n",
    "    (results[\"result_type\"] == DOWNSTREAM_RESULT_TYPE) & \n",
    "    (results[\"metric\"].isin([\"F1_macro\", \"RMSE\"]) &\n",
    "    (results[\"strategy\"] == STRATEGY_TYPE))\n",
    "]\n",
    "\n",
    "# remove experiments where imputation failed\n",
    "downstream_results = downstream_results.merge(\n",
    "    na_impute_results,\n",
    "    how = \"left\",\n",
    "    validate = \"one_to_one\",\n",
    "    indicator = True,\n",
    "    suffixes=(\"\", \"_imp\"),\n",
    "    on = [\"experiment\", \"imputer\", \"task\", \"missing_type\", \"missing_fraction\", \"strategy\", \"column\"]\n",
    ")\n",
    "downstream_results = downstream_results[downstream_results[\"_merge\"]==\"left_only\"]\n",
    "\n",
    "assert len(results[\"strategy\"].unique()) == 1\n",
    "downstream_results.drop([\"experiment\", \"strategy\", \"result_type_imp\", \"metric_imp\", \"train\", \"test\", \"train_imp\", \"test_imp\", \"_merge\"], axis=1, inplace=True)\n",
    "\n",
    "downstream_results = downstream_results.rename(\n",
    "    {\n",
    "        \"imputer\": \"Imputation_Method\",\n",
    "        \"task\": \"Task\",\n",
    "        \"missing_type\": \"Missing Type\",\n",
    "        \"missing_fraction\": \"Missing Fraction\",\n",
    "        \"column\": \"Column\",\n",
    "        \"baseline\": \"Baseline\",\n",
    "        \"imputed\": \"Imputed\",\n",
    "        \"corrupted\": \"Corrupted\"\n",
    "    },\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suitable-leonard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rename_imputer_dict = {\n",
    "    \"ModeImputer\": \"Mean/Mode\",\n",
    "    \"KNNImputer\": \"KNN\",\n",
    "    \"ForestImputer\": \"Random Forest\",\n",
    "    \"AutoKerasImputer\": \"Discriminative DL\",\n",
    "    \"VAEImputer\": \"VAE\",\n",
    "    \"GAINImputer\": \"GAIN\"    \n",
    "}\n",
    "\n",
    "rename_metric_dict = {\n",
    "    \"F1_macro\": CLF_METRIC,\n",
    "    \"RMSE\": REG_METRIC\n",
    "}\n",
    "\n",
    "downstream_results = downstream_results.replace(rename_imputer_dict)\n",
    "downstream_results = downstream_results.replace(rename_metric_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c84d88-7056-4cd4-9dd1-1c1480db39b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-acoustic",
   "metadata": {},
   "source": [
    "### Robustness: Check which Imputers Yielded `NaN`Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "choice-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in downstream_results.columns:\n",
    "    na_sum = downstream_results[col].isna().sum()\n",
    "    if na_sum > 0:\n",
    "        print(\"-----\" * 10)        \n",
    "        print(col, na_sum)\n",
    "        print(\"-----\" * 10)        \n",
    "        na_idx = downstream_results[col].isna()\n",
    "        print(downstream_results.loc[na_idx, \"Imputation Method\"].value_counts(dropna=False))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033acc8a-cfd7-432c-b550-78ed8d9f0c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727db58-daf3-4e06-b42c-287c69889b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf2135-8a6c-49bb-b45f-d8f2b17a6432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29593472-49c2-491c-8228-035f0cb82256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348b244b",
   "metadata": {},
   "source": [
    "## Adding Dataset Info, Sorting and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downstream_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de457480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sorting of data\n",
    "\n",
    "#adjust order to fit the processing time -> fastest first\n",
    "methods_order = CategoricalDtype(['Mean/Mode', 'KNN', 'Random Forest', 'VAE',  'GAIN', 'Discriminative DL'], ordered=True)\n",
    "downstream_results_full_sort = downstream_results.copy()\n",
    "\n",
    "downstream_results_full_sort['Imputation_Method'] = downstream_results_full_sort['Imputation_Method'].astype(methods_order)\n",
    "downstream_results_full_sort = downstream_results_full_sort.sort_values(['Task', 'Missing Type',\n",
    "                                                                         'Missing Fraction', 'Imputed','Imputation_Method'], ascending=[True, True, True, True, True])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cbbba-f877-4c70-94cb-0367001f9aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72092088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataset information from other csv file\n",
    "\n",
    "dataset_info = pd.read_csv('../../datasets_information_overview.csv')\n",
    "dataset_info = dataset_info.rename(columns={\"did\": \"Task\"})\n",
    "\n",
    "downstream_results_full_sort = pd.merge(downstream_results_full_sort, dataset_info, on='Task')\n",
    "#downstream_results_full_sort.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566853a-155e-4dce-bede-ea83f04fc913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f7fca-ad74-45c9-b26f-a9a571a40f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d555e125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ranking of downstream performance per data constellation for every imputation method\n",
    "\n",
    "EXPERIMENTAL_CONDITIONS = [\"Task\", \"Missing Type\", \"Missing Fraction\", \"Column\", \"result_type\"]\n",
    "\n",
    "downstream_results_rank = downstream_results_full_sort.copy()\n",
    "downstream_results_rank[\"Downstream Performance Rank\"] = downstream_results_rank.groupby(EXPERIMENTAL_CONDITIONS).rank(ascending=False, na_option=\"bottom\", method=\"first\")[\"Imputed\"]\n",
    "\n",
    "\n",
    "# create csv for detailled checks\n",
    "downstream_results_rank.to_csv('downstream_results_multi_complete_overview1.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec002ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust column type for Imputation_Method\n",
    "downstream_results_rank['Imputation_Method'] = downstream_results_rank['Imputation_Method'].astype('object')\n",
    "\n",
    "#downstream_results_rank.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b21865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the two columns \"Missing Type\" and \"Missing Fraction\"\n",
    "\n",
    "downstream_results_rank['Missing Type'] = downstream_results_rank['Missing Type'].astype(str)\n",
    "downstream_results_rank['Missing Fraction'] = downstream_results_rank['Missing Fraction'].astype(str)\n",
    "#datatype_new = downstream_results_rank.dtypes\n",
    "\n",
    "downstream_results_rank['Data_Constellation'] = downstream_results_rank['Missing Type'] + ' - ' + downstream_results_rank['Missing Fraction']\n",
    "#downstream_results_rank.to_csv('downstream_results_rank_temp.csv')\n",
    "downstream_results_rank_heatmap2 = downstream_results_rank.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6321d8",
   "metadata": {},
   "source": [
    "## Analyzing Performance Based on Rank per Data Constellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4393619a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Data Constellations\n",
      "_____________________\n",
      "1.0    11\n",
      "2.0     7\n",
      "Name: Downstream Performance Rank, dtype: int64\n",
      "_____________________\n",
      "_____________________\n",
      "KNN              7\n",
      "Random Forest    4\n",
      "Name: Imputation_Method, dtype: int64\n",
      "_____________________\n",
      "7 Amount of results available\n",
      "1.0    4\n",
      "2.0    3\n",
      "Name: Downstream Performance Rank, dtype: int64\n",
      "Average Rank for Random Forest is 1.4285714285714286\n",
      "_____________________\n",
      "11 Amount of results available\n",
      "1.0    7\n",
      "2.0    4\n",
      "Name: Downstream Performance Rank, dtype: int64\n",
      "Average Rank for KNN is 1.3636363636363635\n",
      "_____________________\n",
      "0 Amount of results available\n",
      "Series([], Name: Downstream Performance Rank, dtype: int64)\n",
      "Average Rank for Mean/Mode is nan\n",
      "_____________________\n",
      "0 Amount of results available\n",
      "Series([], Name: Downstream Performance Rank, dtype: int64)\n",
      "Average Rank for VAE is nan\n",
      "_____________________\n",
      "0 Amount of results available\n",
      "Series([], Name: Downstream Performance Rank, dtype: int64)\n",
      "Average Rank for GAIN is nan\n",
      "_____________________\n",
      "0 Amount of results available\n",
      "Series([], Name: Downstream Performance Rank, dtype: int64)\n",
      "Average Rank for Discriminative DL is nan\n",
      "_____________________\n"
     ]
    }
   ],
   "source": [
    "data = downstream_results_rank.copy()\n",
    "\n",
    "# Count amount of different Data constellations in column \"Data_Constellation\"\n",
    "dc_unique = data.Data_Constellation.unique().size\n",
    "print(dc_unique, \"Data Constellations\")\n",
    "print(\"_____________________\")\n",
    "# Count amount of 1.0 Ranking result in column \"Downstream Performance Rank\" \n",
    "rank_count = data['Downstream Performance Rank'].value_counts()\n",
    "print(rank_count)\n",
    "print(\"_____________________\")\n",
    "# Filter for 1.0 Ranking -> Overview -> save as csv\n",
    "rank_1 = data.loc[data['Downstream Performance Rank'] == 1.0]\n",
    "rank_1.to_csv('rank_1.csv')\n",
    "\n",
    "print(\"_____________________\")\n",
    "# Count how often each Imputation Method is present -> most \"wins\"\n",
    "rank_wins = rank_1['Imputation_Method'].value_counts()\n",
    "print(rank_wins)\n",
    "print(\"_____________________\")\n",
    "\n",
    "# BE AWARE THAT THE AVERAGE RANK DOES NOT CONSIDER MISSING RESULTS, WHICH RESULT IN THE WORST RANK BY DEFAULT\n",
    "# Take initial overview and filter for each imputation method and calculate average rank and average improvement\n",
    "methods = ['Random Forest', 'KNN', 'Mean/Mode', 'VAE', 'GAIN', 'Discriminative DL']\n",
    "for i in methods:\n",
    "    df_average_rank = data.loc[data['Imputation_Method'] == i]\n",
    "    len_ar = len(df_average_rank)\n",
    "    print(len_ar, \"Amount of results available\")\n",
    "    rank_pos = df_average_rank['Downstream Performance Rank'].value_counts().sort_index(ascending=True)\n",
    "    print(rank_pos)\n",
    "    average_rank = df_average_rank[\"Downstream Performance Rank\"].mean()\n",
    "    print(\"Average Rank for\", i, \"is\", average_rank)\n",
    "    #average_improvement = df_average_rank[\"Improvement\"].mean()\n",
    "    #print(\"Average Improvement to baseline is\", average_improvement)\n",
    "    print(\"_____________________\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f32ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank_1_backup = rank_1.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab956527-74a7-4fff-8837-af5558b8587c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6154e16-9fdf-473d-8665-4d40a41f0db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7525131654160798"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_1[\"Imputed\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee9beb55-7f9b-419e-9a84-85bde69d21a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7449352011162976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_1[\"Baseline\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddc55b-c352-4213-8568-077ff92e586b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "750b97d4",
   "metadata": {},
   "source": [
    "## Set Average Best Imputation Method Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07826576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET AVERAGE BEST IMPUTATION METHOD HERE, BASED ON THE PREVIOUS RESULTS\n",
    "# Alternatively you can define a baseline method here, which will be used instead, depending on your analysis goals\n",
    "\n",
    "AVERAGE_BEST_IMPUTATION_METHOD = \"KNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d5f43d8-9911-4ca8-a94f-ad383e50796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data[\"Imputed\"]<0.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ca8b8d-0cab-46de-b4c9-7037e7085f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8bf3d-9b79-4e3a-aa5f-dc0953a57750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a38d0e6f",
   "metadata": {},
   "source": [
    "## Differences in Performance Relative to Average Best Imputation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8e295fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Petro\\AppData\\Local\\Temp\\ipykernel_2464\\4275860856.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  av_best['Task'] = av_best['Task'].astype(str)\n",
      "C:\\Users\\Petro\\AppData\\Local\\Temp\\ipykernel_2464\\4275860856.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  av_best['Data_Constellation'] = av_best['Data_Constellation'] + ' - ' + av_best['Task']\n",
      "C:\\Users\\Petro\\AppData\\Local\\Temp\\ipykernel_2464\\4275860856.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rank_1['Task'] = rank_1['Task'].astype(str)\n",
      "C:\\Users\\Petro\\AppData\\Local\\Temp\\ipykernel_2464\\4275860856.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rank_1['Data_Constellation'] = rank_1['Data_Constellation'] + ' - ' + rank_1['Task']\n"
     ]
    }
   ],
   "source": [
    "av_best = data.loc[data['Imputation_Method'] == AVERAGE_BEST_IMPUTATION_METHOD]\n",
    "av_best['Task'] = av_best['Task'].astype(str)\n",
    "av_best['Data_Constellation'] = av_best['Data_Constellation'] + ' - ' + av_best['Task']\n",
    "\n",
    "av_best = av_best[['Imputation_Method', 'Imputed', 'Data_Constellation', 'Downstream Performance Rank']]\n",
    "av_best = av_best.rename(columns={'Imputation_Method':'Imputation_Method_average', \n",
    "                               'Imputed':'Imputed_average',\n",
    "                                 'Downstream Performance Rank':'Downstream Performance Rank Average'})\n",
    "rank_1 = data.loc[data['Downstream Performance Rank'] == 1.0]\n",
    "rank_1['Task'] = rank_1['Task'].astype(str)\n",
    "rank_1['Data_Constellation'] = rank_1['Data_Constellation'] + ' - ' + rank_1['Task']\n",
    "rank_1 = rank_1[['Imputation_Method', 'Imputed', 'Data_Constellation', 'Downstream Performance Rank']]\n",
    "rank_1 = rank_1.rename(columns={'Imputation_Method':'Imputation_Method_best', \n",
    "                               'Imputed':'Imputed_best',\n",
    "                               'Downstream Performance Rank':'Downstream Performance Rank Best'})\n",
    "\n",
    "performance_difference = pd.merge(av_best, rank_1, on='Data_Constellation')\n",
    "#performance_difference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90455724-88f3-4ad8-a637-29c964e867a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd83619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Difference in Improvement from best method to average best method for F1 0.010792014158396007\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference between the best imputation method for each data constellation to the average best imputation method in F1 score\n",
    "\n",
    "performance_difference['Performance Difference Best to Average'] = performance_difference['Imputed_best'] - performance_difference['Imputed_average']\n",
    "Average_Difference = performance_difference['Performance Difference Best to Average'].mean()\n",
    "print(\"Average Difference in Improvement from best method to average best method for F1\", Average_Difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f547f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on F1 Score the Average best method is worse than the best method by this percentage 1.722225038698488\n"
     ]
    }
   ],
   "source": [
    "# Improvement by Percentage\n",
    "\n",
    "performance_difference['Performance Difference Best to Average in Percentage'] = ((performance_difference['Imputed_best'] - performance_difference['Imputed_average'])/performance_difference['Imputed_best'])*100\n",
    "Average_Difference_per = performance_difference['Performance Difference Best to Average in Percentage'].mean()\n",
    "\n",
    "print(\"Based on F1 Score the Average best method is worse than the best method by this percentage\", Average_Difference_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4da11dda-76aa-4670-9ab4-8035944c6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_difference.to_csv('performance_difference.csv')\n",
    "#performance_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ad6ef-c559-46b9-a79b-8926ec5eae3d",
   "metadata": {},
   "source": [
    "# Differences in Performance Relative to Worst Imputation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "109aed8c-9d59-4fed-bef9-e69846f793d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Petro\\AppData\\Local\\Temp\\ipykernel_2464\\2452656571.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rank_5['Task'] = rank_5['Task'].astype(str)\n",
      "C:\\Users\\Petro\\AppData\\Local\\Temp\\ipykernel_2464\\2452656571.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rank_5['Data_Constellation'] = rank_5['Data_Constellation'] + ' - ' + rank_5['Task']\n"
     ]
    }
   ],
   "source": [
    "rank_5 = data.loc[data['Downstream Performance Rank'] == 2.0]\n",
    "rank_5['Task'] = rank_5['Task'].astype(str)\n",
    "rank_5['Data_Constellation'] = rank_5['Data_Constellation'] + ' - ' + rank_5['Task']\n",
    "rank_5 = rank_5[['Imputation_Method', 'Imputed', 'Data_Constellation', 'Downstream Performance Rank']]\n",
    "rank_5 = rank_5.rename(columns={'Imputation_Method':'Imputation_Method_worst', \n",
    "                               'Imputed':'Imputed_worst',\n",
    "                               'Downstream Performance Rank':'Downstream Performance Rank worst'})\n",
    "\n",
    "\n",
    "\n",
    "performance_difference = pd.merge(rank_5, rank_1, on='Data_Constellation')\n",
    "#performance_difference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d309bd-291f-404a-9aa1-66e463e92773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f8eb1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Difference in Improvement from best method to average best method for F1 0.019610110208866507\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference between the best imputation method for each data constellation to the average best imputation method in F1 score\n",
    "\n",
    "performance_difference['Performance Difference Best to Average'] = performance_difference['Imputed_best'] - performance_difference['Imputed_worst']\n",
    "Average_Difference = performance_difference['Performance Difference Best to Average'].mean()\n",
    "print(\"Average Difference in Improvement from best method to average best method for F1\", Average_Difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97d09843-70d1-45a2-a598-9683ba1f8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on F1 Score the Average best method is worse than the best method by this percentage 4.673981743347371\n"
     ]
    }
   ],
   "source": [
    "# Improvement by Percentage\n",
    "\n",
    "performance_difference['Performance Difference Best to Average in Percentage'] = ((performance_difference['Imputed_best'] - performance_difference['Imputed_worst'])/performance_difference['Imputed_best'])*100\n",
    "Average_Difference_per = performance_difference['Performance Difference Best to Average in Percentage'].mean()\n",
    "\n",
    "print(\"Based on F1 Score the Average best method is worse than the best method by this percentage\", Average_Difference_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c553dd-7a08-4880-8721-a1904549cba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cf4ee6d-a0b0-402d-9fc1-3ca7a22361c9",
   "metadata": {},
   "source": [
    "# Difference between jäger mean/mode and mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a89053-277b-42c9-be1c-a4ccbea7d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mode = data.loc[data['Imputation_Method'] == \"Mean/Mode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266e0fd-53e2-4093-81e6-e04b991b9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_mode['Performance Difference'] = mean_mode['Imputed'] - mean_mode['Baseline']\n",
    "Average_Difference = mean_mode['Performance Difference'].mean()\n",
    "print(\"Difference from jäger and mine mean mode\", Average_Difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9612444-f3fd-4134-a83d-2407cbc8d5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba4b7b0",
   "metadata": {},
   "source": [
    "## Analysis and Ranking based on F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002edff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Relative Difference in Percent -> Best Method to Average Best Method\n",
    "\n",
    "data = downstream_results_rank.copy()\n",
    "data['Task'] = data['Task'].astype(str)\n",
    "data['Data_Constellation_full'] = data['Data_Constellation'] + ' - ' + data['Task']\n",
    "\n",
    "\n",
    "dc_unique = data.Data_Constellation_full.unique()\n",
    "\n",
    "data_constellations = dc_unique.tolist()\n",
    "methods = ['Random Forest', 'KNN', 'Mean/Mode', 'VAE', 'GAIN', 'Discriminative DL']\n",
    "average_best_complete = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in data_constellations:\n",
    "    data_constel = data.loc[data['Data_Constellation_full'] == i]\n",
    "    best_score = data_constel.loc[data_constel['Downstream Performance Rank'] == 1.0]\n",
    "    average_best = data_constel.loc[data_constel['Imputation_Method'] == AVERAGE_BEST_IMPUTATION_METHOD]\n",
    "    best_score_int = best_score.iloc[0]['Imputed']\n",
    "    average_best_int = average_best.iloc[0]['Imputed']\n",
    "    calc_result = ((best_score_int - average_best_int)/average_best_int)\n",
    "    average_best['Performance Difference to Best to Average in Percent'] = calc_result\n",
    "    average_best_complete = average_best_complete.append(average_best)\n",
    "\n",
    "average_best_complete\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference in Percentage\n",
    "average_difference = average_best_complete['Performance Difference to Best to Average in Percent'].mean()\n",
    "print(average_difference, \"average difference in Percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebae3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Relative Difference in absolute values (F1 Score) -> Best Method to Average Best Method\n",
    "\n",
    "data = downstream_results_rank.copy()\n",
    "data['Task'] = data['Task'].astype(str)\n",
    "data['Data_Constellation_full'] = data['Data_Constellation'] + ' - ' + data['Task']\n",
    "\n",
    "dc_unique = data.Data_Constellation_full.unique()\n",
    "#print(dc_unique)\n",
    "\n",
    "data_constellations = dc_unique.tolist()\n",
    "methods = ['Random Forest', 'KNN', 'Mean/Mode', 'VAE', 'GAIN', 'Discriminative DL']\n",
    "#print(data_constellations)\n",
    "#print(type(methods))\n",
    "average_best_total = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in data_constellations:\n",
    "    data_constel = data.loc[data['Data_Constellation_full'] == i]\n",
    "    best_score = data_constel.loc[data_constel['Downstream Performance Rank'] == 1.0]\n",
    "    average_best = data_constel.loc[data_constel['Imputation_Method'] == AVERAGE_BEST_IMPUTATION_METHOD]\n",
    "    best_score_int = best_score.iloc[0]['Imputed']\n",
    "    average_best_int = average_best.iloc[0]['Imputed']\n",
    "    calc_result = (best_score_int - average_best_int)\n",
    "\n",
    "    average_best['Performance Difference to Best to Average in absolute'] = calc_result\n",
    "    average_best_total = average_best_total.append(average_best)\n",
    " \n",
    "average_best_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81546ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_difference = average_best_total['Performance Difference to Best to Average in absolute'].mean()\n",
    "print(average_difference, \"average difference in absolut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18195773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ab650",
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_results_rank.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902ac71",
   "metadata": {},
   "source": [
    "## Heatmap to Show Detailled Performance of Each Imputation Method for Each Data Constellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071be4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_heat = downstream_results_rank.copy()\n",
    "df_heat.drop([\"Missing Type\", \"Missing Fraction\", \"Column\", \"result_type\", \"metric\", \"Baseline\", \"Corrupted\", \"Unnamed: 0\", \"Unnamed: 0\", \"name\", \"NumberOfClasses\", \"MajorityClassSize\", \"MinorityClassSize\"], axis=1, inplace=True)\n",
    "df_heat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d3f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Heatmap for total F1 score for each data constellation for each method\n",
    "\n",
    "df_heat = df_heat.astype({\"Task\":\"string\"})\n",
    "\n",
    "data_constellations = ['MAR - 0.01', 'MAR - 0.1', 'MAR - 0.3', 'MCAR - 0.5', 'MCAR - 0.01', 'MCAR - 0.1', 'MCAR - 0.3', 'MCAR - 0.5', 'MNAR - 0.01', 'MNAR - 0.1', 'MNAR - 0.3', 'MNAR - 0.5']\n",
    "\n",
    "\n",
    "for i in data_constellations:\n",
    "    data_constel = df_heat.loc[df_heat['Data_Constellation'] == i]\n",
    "\n",
    "    ### uncomment whatever you want to investigate\n",
    "\n",
    "    ## sort by amount datapoints (ascending)\n",
    "    data_constel = data_constel.sort_values(by=['NumberOfInstances'])\n",
    "\n",
    "    ## sort by amount of features (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfFeatures'])\n",
    "\n",
    "    ## sort by amount of datapoints and features (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfInstances', 'NumberOfFeatures'])\n",
    "\n",
    "    ## sort by amount of categorical features and datapoints (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfCategoricalFeatures', 'NumberOfInstances'])\n",
    "\n",
    "    ## sort by amount of numerical features and datapoints (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfNumericFeatures', 'NumberOfInstances'])\n",
    "    \n",
    "    Dataset_number = data_constel[\"Task\"]\n",
    "    Imputation_Method = data_constel[\"Imputation_Method\"]\n",
    "    F1_Score = data_constel[\"Imputed\"]\n",
    "    \n",
    "\n",
    "    trace = go.Heatmap(\n",
    "                   z=F1_Score,\n",
    "                   x=Dataset_number,\n",
    "                   y=Imputation_Method,\n",
    "                   type = 'heatmap',\n",
    "                    autocolorscale= False,\n",
    "                    colorscale = 'Reds',\n",
    "                    zmin=0, zmax=1\n",
    "                    )\n",
    "    data = [trace]\n",
    "    fig = go.Figure(data=data)\n",
    "    fig.update_layout(\n",
    "        title=i,\n",
    "        xaxis_nticks=36)\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heat_dif = downstream_results_rank_heatmap2.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b179fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Difference for every Imputation towards average best Imputation Method per Data Constellation\n",
    "# Calculation for F1 Score Differences (not Percentage)\n",
    "\n",
    "data = downstream_results_rank.copy()\n",
    "data['Task'] = data['Task'].astype(str)\n",
    "data['Data_Constellation_full'] = data['Data_Constellation'] + ' - ' + data['Task']\n",
    "\n",
    "dc_unique = data.Data_Constellation_full.unique()\n",
    "#print(dc_unique)\n",
    "\n",
    "data_constellations = dc_unique.tolist()\n",
    "methods = ['Random Forest', 'KNN', 'Mean/Mode', 'VAE', 'GAIN', 'Discriminative DL']\n",
    "heatmap_data_difference = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in data_constellations:\n",
    "    data_constel = data.loc[data['Data_Constellation_full'] == i]\n",
    "    average_best = data_constel.loc[data_constel['Imputation_Method'] == AVERAGE_BEST_IMPUTATION_METHOD]\n",
    "    dataset_number = best_score.iloc[0]['Task']\n",
    "    for i in methods:\n",
    "        if ((data_constel['Imputation_Method'] == i).any()):\n",
    "            current_score_row = data_constel.loc[data['Imputation_Method'] == i]\n",
    "            current_score_int = current_score_row.iloc[0]['Imputed']\n",
    "            average_best_int = average_best.iloc[0]['Imputed']\n",
    "            calc_result = (current_score_int - average_best_int)\n",
    "            \n",
    "            current_score_row['Performance Difference to Average Best'] = calc_result\n",
    "            heatmap_data_difference = heatmap_data_difference.append(current_score_row)  \n",
    "        else:\n",
    "            print(\"Imputation Method not here ---------------------\")\n",
    "\n",
    "heatmap_data_difference\n",
    "\n",
    "heatmap_data_difference['Missing Type'] = heatmap_data_difference['Missing Type'].astype(str)\n",
    "heatmap_data_difference['Missing Fraction'] = heatmap_data_difference['Missing Fraction'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112aadf-706e-4e85-ba6d-1651b2706e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7512912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Heatmap for F1 score differences for each data constellation for each method relative to average best imputation method\n",
    "\n",
    "heatmap_data_difference = heatmap_data_difference.astype({\"Task\":\"string\"})\n",
    "data_constellations = ['MAR - 0.01', 'MAR - 0.1', 'MAR - 0.3', 'MAR - 0.5', 'MCAR - 0.01', 'MCAR - 0.1', 'MCAR - 0.3', 'MCAR - 0.5', 'MNAR - 0.01', 'MNAR - 0.1', 'MNAR - 0.3', 'MNAR - 0.5']\n",
    "\n",
    "for i in data_constellations:\n",
    "    data_constel = heatmap_data_difference.loc[df_heat['Data_Constellation'] == i]\n",
    "\n",
    "    ### uncomment whatever you want to investigate\n",
    "\n",
    "    ## sort by amount datapoints (ascending)\n",
    "    data_constel = data_constel.sort_values(by=['NumberOfInstances'])\n",
    "\n",
    "    ## sort by amount of features (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfFeatures'])\n",
    "\n",
    "    ## sort by amount of datapoints and features (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfInstances', 'NumberOfFeatures'])\n",
    "\n",
    "    ## sort by amount of categorical features and datapoints (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfCategoricalFeatures', 'NumberOfInstances'])\n",
    "\n",
    "    ## sort by amount of numerical features and datapoints (ascending)\n",
    "    #data_constel = data_constel.sort_values(by=['NumberOfNumericFeatures', 'NumberOfInstances'])\n",
    "    \n",
    "    Dataset_number = data_constel[\"Task\"]\n",
    "    Imputation_Method = data_constel[\"Imputation_Method\"]\n",
    "    Improvement = data_constel[\"Performance Difference to Average Best\"]\n",
    "    \n",
    "\n",
    "    trace = go.Heatmap(\n",
    "                   z=Improvement,\n",
    "                   x=Dataset_number,\n",
    "                   y=Imputation_Method,\n",
    "                   type = 'heatmap',\n",
    "                    autocolorscale= False,\n",
    "                    colorscale = 'RdBu_r',\n",
    "                    zmid=0,\n",
    "                    zmin=(-0.14),\n",
    "                    zmax=0.14,\n",
    "                    )\n",
    "    data = [trace]\n",
    "    fig = go.Figure(data=data)\n",
    "    fig.update_layout(\n",
    "        title=i,\n",
    "        xaxis_nticks=36)\n",
    "    fig.show()\n",
    "    fig.write_image(\"multi_heatmap_f1_score_improvement_to_avbest%s.pdf\" %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cf492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatmap_data_difference.agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data_difference\n",
    "heatmap_data_difference.to_csv('multi_imputed_full_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ab0c1",
   "metadata": {},
   "source": [
    "## Improvment Proportions for All Data Constellations and Methods Relative to Average Best Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing here\n",
    "df_quantiles = heatmap_data_difference.copy()\n",
    "df_quantiles = df_quantiles.drop(df_quantiles[df_quantiles[\"Imputation_Method\"] == AVERAGE_BEST_IMPUTATION_METHOD].index)\n",
    "\n",
    "df_10 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] > (-0.09))].index)\n",
    "df_09 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (-0.09)) | (df_quantiles[\"Performance Difference to Average Best\"] > (-0.07))].index)\n",
    "df_07 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (-0.07)) | (df_quantiles[\"Performance Difference to Average Best\"] > (-0.05))].index)\n",
    "df_05 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (-0.05)) | (df_quantiles[\"Performance Difference to Average Best\"] > (-0.03))].index)\n",
    "df_03 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (-0.03)) | (df_quantiles[\"Performance Difference to Average Best\"] > (-0.01))].index)\n",
    "df_01 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (-0.01)) | (df_quantiles[\"Performance Difference to Average Best\"] > (0.01))].index)\n",
    "df01 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (0.01)) | (df_quantiles[\"Performance Difference to Average Best\"] > (0.03))].index)\n",
    "df03 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (0.03)) | (df_quantiles[\"Performance Difference to Average Best\"] > (0.05))].index)\n",
    "df05 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (0.05)) | (df_quantiles[\"Performance Difference to Average Best\"] > (0.07))].index)\n",
    "df07 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (0.07)) | (df_quantiles[\"Performance Difference to Average Best\"] > (0.09))].index)\n",
    "df09 = df_quantiles.drop(df_quantiles[(df_quantiles[\"Performance Difference to Average Best\"] <= (0.09))].index)\n",
    "\n",
    "#df_quantiles\n",
    "#df_quantiles.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df_10 = len(df_10.index)\n",
    "len_df_09 = len(df_09.index)\n",
    "len_df_07 = len(df_07.index)\n",
    "len_df_05 = len(df_05.index)\n",
    "len_df_03 = len(df_03.index)\n",
    "len_df_01 = len(df_01.index)\n",
    "len_df01 = len(df01.index)\n",
    "len_df03 = len(df03.index)\n",
    "len_df05 = len(df05.index)\n",
    "len_df07 = len(df07.index)\n",
    "len_df09 = len(df09.index)\n",
    "\n",
    "quantile_freq = []\n",
    "\n",
    "quantile_freq.extend((len_df_10, len_df_09, len_df_07, len_df_05, len_df_03, len_df_01, len_df01, len_df03, len_df05, len_df07, len_df09))\n",
    "print(quantile_freq)\n",
    "\n",
    "\n",
    "quantiles = []\n",
    "quantiles.extend(['less than -0.09', '-0.09 to -0.07', '-0.07 to -0.05', '-0.05 to -0.03','-0.03 to -0.01', '-0.01 to 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09'])\n",
    "print(quantiles)\n",
    "\n",
    "improvement_quantiles = pd.DataFrame(\n",
    "    {'Improvement to Average Best': quantiles,\n",
    "     'Amount': quantile_freq,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9523b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(improvement_quantiles, x='Improvement to Average Best', y='Amount')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1315a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split barchart stacks into methods\n",
    "\n",
    "quantile_datasets = [df_10, df_09, df_07, df_05, df_03, df_01, df01, df03, df05, df07, df09]\n",
    "\n",
    "methods = ['Random Forest', 'KNN', 'Mean/Mode', 'VAE', 'GAIN', 'Discriminative DL']\n",
    "methods.remove(AVERAGE_BEST_IMPUTATION_METHOD)\n",
    "print(methods)\n",
    "\n",
    "forest_freq = []\n",
    "knn_freq = []\n",
    "mode_freq = []\n",
    "dl_freq = []\n",
    "vae_freq = []\n",
    "gain_freq = []\n",
    "#print(quantile_datasets)\n",
    "\n",
    "for i in methods:\n",
    "    for j in quantile_datasets:\n",
    "\n",
    "        df_temp = j.copy()\n",
    "        df_temp = df_temp[df_temp['Imputation_Method'].str.contains(i)]\n",
    "\n",
    "        df_temp_len = len(df_temp.index)\n",
    "        if (i == 'Random Forest'):\n",
    "            forest_freq.append(df_temp_len)\n",
    "        elif (i == 'KNN'):\n",
    "            knn_freq.append(df_temp_len)                                       \n",
    "        elif (i == 'Mean/Mode'):\n",
    "            mode_freq.append(df_temp_len)                                                 \n",
    "        elif (i == 'Discriminative DL'):\n",
    "            dl_freq.append(df_temp_len)                                       \n",
    "        elif (i == 'VAE'):\n",
    "            vae_freq.append(df_temp_len)                                         \n",
    "        elif (i == 'GAIN'):\n",
    "            gain_freq.append(df_temp_len)                                          \n",
    "                                       \n",
    "print(forest_freq)\n",
    "print(knn_freq)\n",
    "print(mode_freq)\n",
    "print(dl_freq)\n",
    "print(vae_freq)\n",
    "print(gain_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = ['less than -0.09', '-0.09 to -0.07', '-0.07 to -0.05', '-0.05 to -0.03','-0.03 to -0.01', '-0.01 to 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09']\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Random Forest', x=quantiles, y=forest_freq),\n",
    "    go.Bar(name='KNN', x=quantiles, y=knn_freq),\n",
    "    go.Bar(name='Mean/Mode', x=quantiles, y=mode_freq),\n",
    "    go.Bar(name='Discriminative DL', x=quantiles, y=dl_freq),\n",
    "    go.Bar(name='VAE', x=quantiles, y=vae_freq),\n",
    "    go.Bar(name='GAIN', x=quantiles, y=gain_freq)\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl_per_method.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0072de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split barchart stacks into methods\n",
    "\n",
    "quantile_datasets = [df_10, df_09, df_07, df_05, df_03, df_01, df01, df03, df05, df07, df09]\n",
    "\n",
    "fractions = ['0.01', '0.1', '0.3', '0.5']\n",
    "#print(fractions)\n",
    "\n",
    "freq_001 = []\n",
    "freq_01 = []\n",
    "freq_03 = []\n",
    "freq_05 = []\n",
    "#print(quantile_datasets)\n",
    "\n",
    "for i in fractions:\n",
    "    for j in quantile_datasets:\n",
    "        df_temp = j.copy()\n",
    "        df_temp = df_temp[df_temp['Missing Fraction'].str.contains(i)]\n",
    "        df_temp_len = len(df_temp.index)\n",
    "        if (i == '0.01'):\n",
    "            freq_001.append(df_temp_len)\n",
    "        elif (i == '0.1'):\n",
    "            freq_01.append(df_temp_len)                                       \n",
    "        elif (i == '0.3'):\n",
    "            freq_03.append(df_temp_len)                                                 \n",
    "        elif (i == '0.5'):\n",
    "            freq_05.append(df_temp_len)                                       \n",
    "                                        \n",
    "                                       \n",
    "print(freq_001)\n",
    "print(freq_01)\n",
    "print(freq_03)\n",
    "print(freq_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = ['less than -0.09', '-0.09 to -0.07', '-0.07 to -0.05', '-0.05 to -0.03','-0.03 to -0.01', '-0.01 to 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09']\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='1% Missing Data', x=quantiles, y=freq_001, marker_color='#FD3216'),\n",
    "    go.Bar(name='10% Missing Data', x=quantiles, y=freq_01, marker_color='#00FE35'),\n",
    "    go.Bar(name='30% Missing Data', x=quantiles, y=freq_03, marker_color='#511CFB'),\n",
    "    go.Bar(name='50% Missing Data', x=quantiles, y=freq_05, marker_color='#FF7F0E'),\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl_per_frac.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb86e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split barchart stacks into methods\n",
    "\n",
    "quantile_datasets = [df_10, df_09, df_07, df_05, df_03, df_01, df01, df03, df05, df07, df09]\n",
    "\n",
    "fractions = ['MCAR', 'MAR', 'MNAR']\n",
    "print(fractions)\n",
    "#print(df_10)\n",
    "\n",
    "freq_001 = []\n",
    "freq_01 = []\n",
    "freq_03 = []\n",
    "#print(quantile_datasets)\n",
    "\n",
    "for i in fractions:\n",
    "    for j in quantile_datasets:\n",
    "        df_temp = j.copy()\n",
    "        df_temp = df_temp[df_temp['Missing Type'].str.contains(i)]\n",
    "        df_temp_len = len(df_temp.index)\n",
    "        if (i == 'MCAR'):\n",
    "            freq_001.append(df_temp_len)\n",
    "        elif (i == 'MAR'):\n",
    "            freq_01.append(df_temp_len)                                       \n",
    "        elif (i == 'MNAR'):\n",
    "            freq_03.append(df_temp_len)                                                                                     \n",
    "                                        \n",
    "                                       \n",
    "print(freq_001)\n",
    "print(freq_01)\n",
    "print(freq_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f025c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = ['less than -0.09', '-0.09 to -0.07', '-0.07 to -0.05', '-0.05 to -0.03','-0.03 to -0.01', '-0.01 to 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09']\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='MCAR', x=quantiles, y=freq_001, marker_color='#222A2A'),\n",
    "    go.Bar(name='MAR', x=quantiles, y=freq_01, marker_color='#B68100'),\n",
    "    go.Bar(name='MNAR', x=quantiles, y=freq_03, marker_color='#750D86'),\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl_per_patt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e1809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe687fb",
   "metadata": {},
   "source": [
    "## Improvment Proportions for the Best Imputation Method per Data Constellation Relative to Average Best Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747384bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "improv_to_av_bar = heatmap_data_difference.copy()\n",
    "\n",
    "improv_to_av_bar = improv_to_av_bar.drop(improv_to_av_bar[improv_to_av_bar[\"Downstream Performance Rank\"] != 1.0].index)\n",
    "\n",
    "df_01 = improv_to_av_bar.drop(improv_to_av_bar[(improv_to_av_bar[\"Performance Difference to Average Best\"] <= (-0.01)) | (improv_to_av_bar[\"Performance Difference to Average Best\"] > (0.01))].index)\n",
    "df01 = improv_to_av_bar.drop(improv_to_av_bar[(improv_to_av_bar[\"Performance Difference to Average Best\"] <= (0.01)) | (improv_to_av_bar[\"Performance Difference to Average Best\"] > (0.03))].index)\n",
    "df03 = improv_to_av_bar.drop(improv_to_av_bar[(improv_to_av_bar[\"Performance Difference to Average Best\"] <= (0.03)) | (improv_to_av_bar[\"Performance Difference to Average Best\"] > (0.05))].index)\n",
    "df05 = improv_to_av_bar.drop(improv_to_av_bar[(improv_to_av_bar[\"Performance Difference to Average Best\"] <= (0.05)) | (improv_to_av_bar[\"Performance Difference to Average Best\"] > (0.07))].index)\n",
    "df07 = improv_to_av_bar.drop(improv_to_av_bar[(improv_to_av_bar[\"Performance Difference to Average Best\"] <= (0.07)) | (improv_to_av_bar[\"Performance Difference to Average Best\"] > (0.09))].index)\n",
    "df09 = improv_to_av_bar.drop(improv_to_av_bar[(improv_to_av_bar[\"Performance Difference to Average Best\"] <= (0.09))].index)\n",
    "\n",
    "improv_to_av_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df_01 = len(df_01.index)\n",
    "len_df01 = len(df01.index)\n",
    "len_df03 = len(df03.index)\n",
    "len_df05 = len(df05.index)\n",
    "len_df07 = len(df07.index)\n",
    "len_df09 = len(df09.index)\n",
    "\n",
    "quantile_freq = []\n",
    "quantile_freq.extend((len_df_01, len_df01, len_df03, len_df05, len_df07, len_df09))\n",
    "print(quantile_freq)\n",
    "\n",
    "\n",
    "quantiles = []\n",
    "quantiles.extend(['less than 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09'])\n",
    "print(quantiles)\n",
    "\n",
    "improvement_quantiles = pd.DataFrame(\n",
    "    {'Improvement to Average Best': quantiles,\n",
    "     'Amount': quantile_freq,\n",
    "    })\n",
    "\n",
    "fig = px.bar(improvement_quantiles, x='Improvement to Average Best', y='Amount')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl_only_best.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split barchart stacks into methods\n",
    "\n",
    "quantile_datasets = [df_01, df01, df03, df05, df07, df09]\n",
    "\n",
    "methods = ['Random Forest', 'KNN', 'Mean/Mode', 'VAE', 'GAIN', 'Discriminative DL']\n",
    "methods.remove(AVERAGE_BEST_IMPUTATION_METHOD)\n",
    "print(methods)\n",
    "\n",
    "forest_freq = []\n",
    "knn_freq = []\n",
    "mode_freq = []\n",
    "dl_freq = []\n",
    "vae_freq = []\n",
    "gain_freq = []\n",
    "#print(quantile_datasets)\n",
    "\n",
    "for i in methods:\n",
    "    for j in quantile_datasets:\n",
    "        df_temp = j.copy()\n",
    "        df_temp = df_temp[df_temp['Imputation_Method'].str.contains(i)]\n",
    "        df_temp_len = len(df_temp.index)\n",
    "        if (i == 'Random Forest'):\n",
    "            forest_freq.append(df_temp_len)\n",
    "        elif (i == 'KNN'):\n",
    "            knn_freq.append(df_temp_len)                                       \n",
    "        elif (i == 'Mean/Mode'):\n",
    "            mode_freq.append(df_temp_len)                                                 \n",
    "        elif (i == 'Discriminative DL'):\n",
    "            dl_freq.append(df_temp_len)                                       \n",
    "        elif (i == 'VAE'):\n",
    "            vae_freq.append(df_temp_len)                                         \n",
    "        elif (i == 'GAIN'):\n",
    "            gain_freq.append(df_temp_len)                                          \n",
    "                                       \n",
    "print(forest_freq)\n",
    "print(knn_freq)\n",
    "print(mode_freq)\n",
    "print(dl_freq)\n",
    "print(vae_freq)\n",
    "print(gain_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284db679",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = ['less than 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09']\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Random Forest', x=quantiles, y=forest_freq),\n",
    "    go.Bar(name='KNN', x=quantiles, y=knn_freq),\n",
    "    go.Bar(name='Mean/Mode', x=quantiles, y=mode_freq),\n",
    "    go.Bar(name='Discriminative DL', x=quantiles, y=dl_freq),\n",
    "    go.Bar(name='VAE', x=quantiles, y=vae_freq),\n",
    "    go.Bar(name='GAIN', x=quantiles, y=gain_freq)\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl_only_best_per_method.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split barchart stacks into missingness fractions\n",
    "\n",
    "quantile_datasets = [df_01, df01, df03, df05, df07, df09]\n",
    "\n",
    "fractions = ['0.01', '0.1', '0.3', '0.5']\n",
    "print(fractions)\n",
    "\n",
    "\n",
    "freq_001 = []\n",
    "freq_01 = []\n",
    "freq_03 = []\n",
    "freq_05 = []\n",
    "#print(quantile_datasets)\n",
    "\n",
    "for i in fractions:\n",
    "    for j in quantile_datasets:\n",
    "        df_temp = j.copy()\n",
    "        df_temp = df_temp[df_temp['Missing Fraction'].str.contains(i)]\n",
    "        df_temp_len = len(df_temp.index)\n",
    "        if (i == '0.01'):\n",
    "            freq_001.append(df_temp_len)\n",
    "        elif (i == '0.1'):\n",
    "            freq_01.append(df_temp_len)                                       \n",
    "        elif (i == '0.3'):\n",
    "            freq_03.append(df_temp_len)                                                 \n",
    "        elif (i == '0.5'):\n",
    "            freq_05.append(df_temp_len)                                       \n",
    "                                        \n",
    "                                       \n",
    "print(freq_001)\n",
    "print(freq_01)\n",
    "print(freq_03)\n",
    "print(freq_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = ['less than 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09']\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='1% Missing Data', x=quantiles, y=freq_001, marker_color='#FD3216'),\n",
    "    go.Bar(name='10% Missing Data', x=quantiles, y=freq_01, marker_color='#00FE35'),\n",
    "    go.Bar(name='30% Missing Data', x=quantiles, y=freq_03, marker_color='#511CFB'),\n",
    "    go.Bar(name='50% Missing Data', x=quantiles, y=freq_05, marker_color='#FF7F0E'),\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl_only_best_per_frac.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split barchart stacks into missingness fractions\n",
    "\n",
    "quantile_datasets = [df_01, df01, df03, df05, df07, df09]\n",
    "\n",
    "fractions = ['MCAR', 'MAR', 'MNAR']\n",
    "print(fractions)\n",
    "\n",
    "\n",
    "freq_001 = []\n",
    "freq_01 = []\n",
    "freq_03 = []\n",
    "#print(quantile_datasets)\n",
    "\n",
    "for i in fractions:\n",
    "    for j in quantile_datasets:\n",
    "        df_temp = j.copy()\n",
    "        df_temp = df_temp[df_temp['Missing Type'].str.contains(i)]\n",
    "        df_temp_len = len(df_temp.index)\n",
    "        if (i == 'MCAR'):\n",
    "            freq_001.append(df_temp_len)\n",
    "        elif (i == 'MAR'):\n",
    "            freq_01.append(df_temp_len)                                       \n",
    "        elif (i == 'MNAR'):\n",
    "            freq_03.append(df_temp_len)                                                                                     \n",
    "                                        \n",
    "                                       \n",
    "print(freq_001)\n",
    "print(freq_01)\n",
    "print(freq_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = ['less than 0.01', '0.01 to 0.03', '0.03 to 0.05', '0.05 to 0.07', '0.07 to 0.09', 'more than 0.09']\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='MCAR', x=quantiles, y=freq_001, marker_color='#222A2A'),\n",
    "    go.Bar(name='MAR', x=quantiles, y=freq_01, marker_color='#B68100'),\n",
    "    go.Bar(name='MNAR', x=quantiles, y=freq_03, marker_color='#750D86'),\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()\n",
    "fig.write_image(\"improv_rel_to_av_all_DC_no_av_incl_only_best_per_patt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbc420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25fcd8dd",
   "metadata": {},
   "source": [
    "## Extract datasets for Automated Imputation Method Selection\n",
    "\n",
    "To Do: Explore the possibility, that the average best method replaces the best method for a data constellation, if the improvement gain for the best method is below 1%\n",
    "\n",
    "### Potential Features:\n",
    "Missingess Pattern (Missing Type)  \n",
    "Missing Fraction (Missing Fraction)  \n",
    "Datapoints (NumberOfInstances)  \n",
    "Features in total (NumberOfFeatures)  \n",
    "Numeric Features (NumberOfNumericFeatures)  \n",
    "Categorical Features (NumberOfCategoricalFeatures)  \n",
    "Downstream Task Type -> Classification/Regression (metric)\n",
    "  \n",
    "    \n",
    "      \n",
    "Label: Best Imputation Method (Imputation_Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataset with only the best method for each data constellation\n",
    "rank_1_backup.to_csv('rank_1_backup.csv')\n",
    "rank_1_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for Training \n",
    "properties_train_dataset_8 = rank_1_backup.copy()\n",
    "properties_train_dataset_8 = properties_train_dataset_8[['Imputation_Method','Missing Type','Missing Fraction',\n",
    "                                                         'NumberOfInstances','NumberOfFeatures','NumberOfNumericFeatures',\n",
    "                                                         'NumberOfCategoricalFeatures','metric']]\n",
    "\n",
    "properties_train_dataset_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02623e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for Training \n",
    "properties_train_dataset_original = rank_1_backup.copy()\n",
    "properties_train_dataset_original = properties_train_dataset_original[['Imputation_Method','Missing Type','Missing Fraction',\n",
    "                                                         'NumberOfInstances','NumberOfNumericFeatures',\n",
    "                                                         'NumberOfCategoricalFeatures']]\n",
    "\n",
    "properties_train_dataset_original\n",
    "properties_train_dataset_original.to_csv('multi_properties_train_dataset_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_train_dataset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for Training -> replace best method with average best if imporvement is below 1%, 2% or 3%\n",
    "\n",
    "alternate_data = heatmap_data_difference.copy()\n",
    "\n",
    "df_temp = alternate_data.loc[(alternate_data['Downstream Performance Rank'] == 1.0) | (alternate_data['Imputation_Method'] == AVERAGE_BEST_IMPUTATION_METHOD)]\n",
    "\n",
    "\n",
    "dc_unique = alternate_data.Data_Constellation_full.unique()\n",
    "data_constellations = dc_unique.tolist()\n",
    "\n",
    "\n",
    "for i in data_constellations:\n",
    "\n",
    "    # define the threshold here!\n",
    "    \n",
    "    df_temp['Downstream Performance Rank'] = np.where((df_temp['Data_Constellation_full'] == i) & (df_temp['Performance Difference to Average Best'] <= 0.03) & (df_temp['Imputation_Method'] != AVERAGE_BEST_IMPUTATION_METHOD), 9.0, df_temp['Downstream Performance Rank'])\n",
    "    df_temp['Downstream Performance Rank'] = np.where((df_temp['Data_Constellation_full'] == i) & (df_temp['Performance Difference to Average Best'] >= 0.03) & (df_temp['Imputation_Method'] != AVERAGE_BEST_IMPUTATION_METHOD), 11.0, df_temp['Downstream Performance Rank'])\n",
    "   \n",
    "df_temp = df_temp.drop(df_temp[df_temp['Downstream Performance Rank'] == 9.0].index) \n",
    "\n",
    "# Sorting of data\n",
    "\n",
    "#adjust order to fit the processing time -> fastest first\n",
    "methods_order = CategoricalDtype(['Random Forest', 'Mean/Mode', 'KNN', 'VAE', 'GAIN', 'Discriminative DL'], ordered=True)\n",
    "\n",
    "\n",
    "df_temp['Imputation_Method'] = df_temp['Imputation_Method'].astype(methods_order)\n",
    "\n",
    "df_temp = df_temp.sort_values(['Data_Constellation_full','Imputation_Method'], ascending=[True, True])\n",
    "df_temp = df_temp.drop_duplicates(subset=[\"Data_Constellation_full\"], keep='last')\n",
    "\n",
    "df_temp = df_temp[['Imputation_Method','Missing Type','Missing Fraction',\n",
    "                                                         'NumberOfInstances','NumberOfNumericFeatures',\n",
    "                                                         'NumberOfCategoricalFeatures']]\n",
    "\n",
    "\n",
    "df_temp.to_csv('multi_properties_train_dataset_3_percent.csv')\n",
    "df_temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_data = heatmap_data_difference.copy()\n",
    "\n",
    "df_temp = alternate_data.loc[(alternate_data['Downstream Performance Rank'] == 1.0) | (alternate_data['Imputation_Method'] == AVERAGE_BEST_IMPUTATION_METHOD)]\n",
    "\n",
    "\n",
    "dc_unique = alternate_data.Data_Constellation_full.unique()\n",
    "data_constellations = dc_unique.tolist()\n",
    "\n",
    "\n",
    "for i in data_constellations:\n",
    "\n",
    "    # define the threshold here!\n",
    "    \n",
    "    df_temp['Downstream Performance Rank'] = np.where((df_temp['Data_Constellation_full'] == i) & (df_temp['Performance Difference to Average Best'] <= 0.02) & (df_temp['Imputation_Method'] != AVERAGE_BEST_IMPUTATION_METHOD), 9.0, df_temp['Downstream Performance Rank'])\n",
    "    df_temp['Downstream Performance Rank'] = np.where((df_temp['Data_Constellation_full'] == i) & (df_temp['Performance Difference to Average Best'] >= 0.02) & (df_temp['Imputation_Method'] != AVERAGE_BEST_IMPUTATION_METHOD), 11.0, df_temp['Downstream Performance Rank'])\n",
    "   \n",
    "df_temp = df_temp.drop(df_temp[df_temp['Downstream Performance Rank'] == 9.0].index) \n",
    "\n",
    "# Sorting of data\n",
    "\n",
    "#adjust order to fit the processing time -> fastest first\n",
    "methods_order = CategoricalDtype(['Random Forest', 'Mean/Mode', 'KNN', 'VAE', 'GAIN', 'Discriminative DL'], ordered=True)\n",
    "\n",
    "\n",
    "df_temp['Imputation_Method'] = df_temp['Imputation_Method'].astype(methods_order)\n",
    "\n",
    "df_temp = df_temp.sort_values(['Data_Constellation_full','Imputation_Method'], ascending=[True, True])\n",
    "df_temp = df_temp.drop_duplicates(subset=[\"Data_Constellation_full\"], keep='last')\n",
    "\n",
    "df_temp = df_temp[['Imputation_Method','Missing Type','Missing Fraction',\n",
    "                                                         'NumberOfInstances','NumberOfNumericFeatures',\n",
    "                                                         'NumberOfCategoricalFeatures']]\n",
    "\n",
    "\n",
    "df_temp.to_csv('multi_properties_train_dataset_2_percent.csv')\n",
    "df_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_data = heatmap_data_difference.copy()\n",
    "\n",
    "df_temp = alternate_data.loc[(alternate_data['Downstream Performance Rank'] == 1.0) | (alternate_data['Imputation_Method'] == AVERAGE_BEST_IMPUTATION_METHOD)]\n",
    "\n",
    "\n",
    "dc_unique = alternate_data.Data_Constellation_full.unique()\n",
    "data_constellations = dc_unique.tolist()\n",
    "\n",
    "\n",
    "for i in data_constellations:\n",
    "\n",
    "    # define the threshold here!\n",
    "    \n",
    "    df_temp['Downstream Performance Rank'] = np.where((df_temp['Data_Constellation_full'] == i) & (df_temp['Performance Difference to Average Best'] <= 0.01) & (df_temp['Imputation_Method'] != AVERAGE_BEST_IMPUTATION_METHOD), 9.0, df_temp['Downstream Performance Rank'])\n",
    "    df_temp['Downstream Performance Rank'] = np.where((df_temp['Data_Constellation_full'] == i) & (df_temp['Performance Difference to Average Best'] >= 0.01) & (df_temp['Imputation_Method'] != AVERAGE_BEST_IMPUTATION_METHOD), 11.0, df_temp['Downstream Performance Rank'])\n",
    "   \n",
    "df_temp = df_temp.drop(df_temp[df_temp['Downstream Performance Rank'] == 9.0].index) \n",
    "\n",
    "# Sorting of data\n",
    "\n",
    "#adjust order to fit the processing time -> fastest first\n",
    "methods_order = CategoricalDtype(['Random Forest', 'Mean/Mode', 'KNN', 'VAE', 'GAIN', 'Discriminative DL'], ordered=True)\n",
    "\n",
    "\n",
    "df_temp['Imputation_Method'] = df_temp['Imputation_Method'].astype(methods_order)\n",
    "\n",
    "df_temp = df_temp.sort_values(['Data_Constellation_full','Imputation_Method'], ascending=[True, True])\n",
    "df_temp = df_temp.drop_duplicates(subset=[\"Data_Constellation_full\"], keep='last')\n",
    "\n",
    "df_temp = df_temp[['Imputation_Method','Missing Type','Missing Fraction',\n",
    "                                                         'NumberOfInstances','NumberOfNumericFeatures',\n",
    "                                                         'NumberOfCategoricalFeatures']]\n",
    "\n",
    "\n",
    "df_temp.to_csv('multi_properties_train_dataset_1_percent.csv')\n",
    "df_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27152b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
